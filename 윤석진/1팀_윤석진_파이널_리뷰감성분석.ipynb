#í•œê¸€ ê¸€ì”¨ í°íŠ¸ ì„¤ì¹˜
%%capture
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')
plt.rcParams['font.family'] = 'NanumGothic'

# í‘œì—ì„œ ('-') ë§ˆì´ë„ˆìŠ¤ í‘œì‹œ
plt.rcParams['axes.unicode_minus'] = False

# =============================================================
# [Step 1] ë°ì´í„° ì¤€ë¹„: ì˜ì–´ ë¦¬ë·° í•„í„°ë§ ë° ê·¸ë£¹í•‘
# =============================================================
import pandas as pd
import numpy as np
import re
from tqdm.auto import tqdm

# --- 1-1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•„ìš”ì‹œ) ---
!pip install -q transformers torch vaderSentiment bertopic konlpy

# --- 1-2. ì›ë³¸ ë°ì´í„° ë¡œë“œ ---
file_path = '/content/S-team_250930-final.csv'
try:
    df_original = pd.read_csv(file_path)
    print(f"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ! (Shape: {df_original.shape})")
except FileNotFoundError:
    print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: '{file_path}' ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

# --- 1-3. ì–¸ì–´ íƒì§€ ë° ì˜ì–´(ë¹„í•œêµ­ì–´) ë°ì´í„° í•„í„°ë§ ---
tqdm.pandas()

def detect_language(text):
    if not isinstance(text, str) or len(text.strip()) == 0: return 'unknown'
    return 'korean' if re.search("[ã„±-íž£]", text) else 'english'

# 'translated_en' ì»¬ëŸ¼ì˜ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì–¸ì–´ íƒì§€
df_original['language_detected'] = df_original['review'].progress_apply(detect_language)

# ðŸ”¥ ë¶„ì„ ëŒ€ìƒì„ 'ì˜ë¬¸ ë¦¬ë·°'ë¡œ í•œì •
df_english = df_original[df_original['language_detected'] != 'korean'].copy()
print(f"\nâœ… ì˜ì–´(ë¹„í•œêµ­ì–´) ë¦¬ë·° í•„í„°ë§ ì™„ë£Œ! (ì´ {len(df_english)}ê°œ)")


# --- 1-4. ì‚¬ìš©ìž ê·¸ë£¹í•‘ ì»¬ëŸ¼ ìƒì„± ---
# Playtime ê¸°ì¤€ ê·¸ë£¹í•‘
df_english['playtime_hours_at_review'] = df_english['author_playtime_at_review'] / 60.0
# 'own'ê³¼ 'trial' ê·¸ë£¹ì„ ë³‘í•©
df_english['playtime_merged'] = df_english['user_groupby_playtime'].replace({
    'own': 'ì´ˆê¸° íƒìƒ‰(0-10h)', 'trial': 'ì´ˆê¸° íƒìƒ‰(0-10h)',
    'normal': 'ì¼ë°˜ ìœ ì €', 'heavy': 'í—¤ë¹„ ìœ ì €', 'core': 'ì½”ì–´ ìœ ì €'
})

print("\n--- ìƒì„±ëœ í”Œë ˆì´íƒ€ìž„ ê·¸ë£¹ ë¶„í¬ ---")
print(df_english['playtime_merged'].value_counts())

# =============================================================
# [Step 2] ê°ì„± ë¶„ì„: VADER ëª¨ë¸ ì ìš©
# =============================================================
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# --- 2-1. VADER ë¶„ì„ê¸° ë¡œë“œ ---
vader_analyzer = SentimentIntensityAnalyzer()

# --- 2-2. ê°ì„± ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ ì •ì˜ ---
def get_vader_score(text):
    if not isinstance(text, str):
        return 0.0
    # VADERëŠ” ì›ë¬¸ í…ìŠ¤íŠ¸ì— ë°”ë¡œ ì ìš©
    return vader_analyzer.polarity_scores(text)['compound']

# --- 2-3. ê°ì„± ì ìˆ˜ ë° ê·¸ë£¹ ì»¬ëŸ¼ ìƒì„± ---
print("\nì˜ì–´ ë¦¬ë·°ì— VADER ê°ì„± ë¶„ì„ ì ìš© ì¤‘...")
df_english['sentiment_score'] = df_english['translated_en'].progress_apply(get_vader_score)

def assign_sentiment_group_strict(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.3:  # ðŸ”¥ ë¶€ì • ê¸°ì¤€ì„ -0.05ì—ì„œ -0.3ìœ¼ë¡œ ê°•í™”
        return 'Negative'
    else:
        return 'Neutral'

# ê°•í™”ëœ ê¸°ì¤€ìœ¼ë¡œ ìž¬ì‹¤í–‰
df_english['sentiment_group'] = df_english['sentiment_score'].apply(assign_sentiment_group_strict)

print("\nâœ… [ê°•í™”ëœ ê¸°ì¤€] ê°ì„± ë¶„ì„ ì™„ë£Œ!")
print("--- ê°ì„± ê·¸ë£¹ ë¶„í¬ ---")
print(df_english['sentiment_group'].value_counts())

# =============================================================
# âœ¨ [ê°œì„ ëœ ì „ëžµ] Step 3: ê³ ë„í™”ëœ LDA í† í”½ ëª¨ë¸ë§ âœ¨
# =============================================================
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS
from sklearn.decomposition import LatentDirichletAllocation
import pandas as pd

# --- ì´ì „ Step 1 & 2ëŠ” ë™ì¼í•˜ê²Œ ì‹¤í–‰í•˜ì—¬ 'df_english' ë°ì´í„°í”„ë ˆìž„ì„ ì¤€ë¹„í•©ë‹ˆë‹¤ ---

print("\n" + "="*60)
print("âœ¨ [ê°œì„ ëœ ë¶„ì„] ê·¸ë£¹ë³„ 'ì°¨ë³„í™”ëœ' ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ (LDA) âœ¨")
print("="*60)

playtime_categories = ['ì´ˆê¸° íƒìƒ‰(0-10h)', 'ì¼ë°˜ ìœ ì €', 'í—¤ë¹„ ìœ ì €', 'ì½”ì–´ ìœ ì €']
# ê·¸ë£¹ë³„ ìµœì¢… ê²°ê³¼ë¥¼ ì €ìž¥í•  ë”•ì…”ë„ˆë¦¬
final_results = {}

# ðŸ”¥ [í•µì‹¬ ê°œì„  1] ê°•ë ¥í•œ ë¶ˆìš©ì–´ ì‚¬ì „ êµ¬ì¶•
# ê¸°ë³¸ ì˜ì–´ ë¶ˆìš©ì–´ + ì¼ë°˜ì ì¸ ê²Œìž„ ìš©ì–´ + ì´ì „ ë¶„ì„ì—ì„œ ë°œê²¬ëœ ë¬´ì˜ë¯¸í•œ ë‹¨ì–´ ëª¨ë‘ ì¶”ê°€
custom_stop_words = list(ENGLISH_STOP_WORDS) + [
    'game', 'games', 'play', 'playing', 'player', 'players', 'just', 'like',
    'really', 'don', 'doesn', 'im', 've', 'time', 'lot', 'way', 'thing',
    'good', 'bad', 'great', 'problem', 'problems', 'issue', 'issues', 'bit', 'even',
    'got', 'didn', 'think', 'thought', 'said', 'saw', 'know', 'want', 'make',
    'actually', 'people', 'going', 'getting', 'little', 'much', 'review', 'steam'
]
# ðŸ”¥ V2: ê°ì • í‘œí˜„ ë° ë²”ìš©ì  ë¶€ì • ë‹¨ì–´ ì¶”ê°€
custom_stop_words_v2 = custom_stop_words + [
    'kill', 'killed', 'die', 'dead', 'death', 'hate',         # í­ë ¥/ì£½ìŒ ê´€ë ¨
    'fuck', 'shit', 'ass', 'crap', 'motherfucking', 'bull', # ìš•ì„¤/ë¹„ì†ì–´
    'bullying', 'bully',                                      # íŠ¹ì • ê²Œìž„ì—ì„œ ë§Žì´ ë‚˜ì˜¨ ë¶ˆìš©ì–´
    'll', 'guy', 'guys', 'man', 'life'                         # ì¼ë°˜ì ì¸ ëª…ì‚¬/ëŒ€ëª…ì‚¬
]

for segment in playtime_categories:
    print(f"\n--- ðŸ‘¥ ê·¸ë£¹: {segment} ---")
    segment_df = df_english[
        (df_english['playtime_merged'] == segment) &
        (df_english['sentiment_group'] == 'Negative') # 'Negative' ê·¸ë£¹ë§Œ ë¶„ì„
    ]

    num_docs = len(segment_df)
    if num_docs < 50: # ë¶„ì„ì˜ ì‹ ë¢°ë„ë¥¼ ìœ„í•´ ìµœì†Œ ë¬¸ì„œ ìˆ˜ë¥¼ 50ìœ¼ë¡œ ìƒí–¥
        print(f"ë¶€ì • ë¦¬ë·° ìˆ˜ê°€ {num_docs}ê°œë¡œ ë¶„ì„í•˜ê¸°ì— ë¶€ì¡±í•©ë‹ˆë‹¤.")
        continue

    docs = segment_df['translated_en'].dropna().astype(str).tolist()
    print(f"ì´ {len(docs)}ê°œì˜ ì˜ì–´ ë¶€ì • ë¦¬ë·°ë¡œ LDA í† í”½ ëª¨ë¸ë§ ì‹œìž‘...")

    try:
        # ðŸ”¥ [í•µì‹¬ ê°œì„  2] Vectorizer íŒŒë¼ë¯¸í„° ìµœì í™”
        # min_df: ë„ˆë¬´ ë“œë¬¼ê²Œ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ ì œê±° (ìµœì†Œ 10ê°œ ë¬¸ì„œ ì´ìƒ ë“±ìž¥)
        # max_df: ë„ˆë¬´ ìžì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ ì œê±° (ìƒìœ„ 85% ì´ìƒ ë“±ìž¥ ë‹¨ì–´ ì œì™¸)
        # ngram_range: ë‹¨ì–´ ë¬¶ìŒ(ì˜ˆ: 'server connection')ì„ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ ì •í™•ë„ í–¥ìƒ
        vectorizer = CountVectorizer(
            stop_words=custom_stop_words_v2,
            min_df=10,
            max_df=0.85,
            ngram_range=(1, 2) # 1ê°œ ë‹¨ì–´ ë° 2ê°œ ì—°ì† ë‹¨ì–´ ëª¨ë‘ ê³ ë ¤
        )
        X = vectorizer.fit_transform(docs)

        # ðŸ”¥ [í•µì‹¬ ê°œì„  3] ì°¾ê³ ìž í•˜ëŠ” í† í”½ì˜ ìˆ˜ë¥¼ 3~5ê°œë¡œ ì¤„ì—¬ ëª…í™•ì„± í™•ë³´
        lda_model = LatentDirichletAllocation(
            n_components=4, # í•µì‹¬ ë¶ˆë§Œ 4ê°œë¥¼ ì°¾ëŠ”ë‹¤ê³  ê°€ì •
            random_state=42,
            learning_method='online', # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ë” ë¹ ë¦„
            n_jobs=-1 # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
        )
        lda_model.fit(X)

        # 3. ê²°ê³¼ í•´ì„ ë° ì €ìž¥
        topics = []
        feature_names = vectorizer.get_feature_names_out()
        for topic_idx, topic in enumerate(lda_model.components_):
            top_keywords = [feature_names[i] for i in topic.argsort()[:-8:-1]] # ìƒìœ„ 7ê°œ í‚¤ì›Œë“œ
            topics.append({
                "Topic #": topic_idx,
                "Keywords": ", ".join(top_keywords)
            })

        results_df = pd.DataFrame(topics)
        final_results[segment] = results_df
        print("\nðŸ”¥ ë°œê²¬ëœ ì£¼ìš” ë¶ˆë§Œ í† í”½:")
        display(results_df)

    except Exception as e:
        print(f"âš ï¸ LDA ëª¨ë¸ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# =============================================================
# âœ¨ [ìµœì¢… ë‹¨ê³„] ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë° í•´ì„ âœ¨
# =============================================================

print("\n\n" + "="*60)
print("âœ¨ ì „ì²´ ê·¸ë£¹ë³„ ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ ìš”ì•½ âœ¨")
print("="*60)

# ì´ì „ LDA ë¶„ì„ì—ì„œ ë„ì¶œëœ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° í† í”½ì— ì˜ë¯¸(ë ˆì´ë¸”)ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
# ì´ í•´ì„ì€ ë¶„ì„ê°€ì˜ ì£¼ê´€ì´ ë“¤ì–´ê°€ëŠ” ë¶€ë¶„ì´ë©°, ë°œí‘œì˜ í•µì‹¬ì ì¸ ì£¼ìž¥ìž…ë‹ˆë‹¤.

# --- 1. ê° ê·¸ë£¹ë³„ í‚¤ì›Œë“œ í•´ì„ ë° ë ˆì´ë¸”ë§ ---
summary_data = {
    'í”Œë ˆì´íƒ€ìž„ ê·¸ë£¹': [
        'ì´ˆê¸° íƒìƒ‰ (0-10h)',
        'ì¼ë°˜ ìœ ì € (10-100h)',
        'í—¤ë¹„ ìœ ì € (100h+)',
        'ì½”ì–´ ìœ ì € (ìˆ˜ë°± ì‹œê°„+)'
    ],
    'í•µì‹¬ ë¶ˆë§Œ (Topic 1)': [
        'ê¸°ëŒ€ì™€ ë‹¤ë¥¸ ì²«ì¸ìƒ (êµ¬ë§¤/ê°€ì¹˜ íŒë‹¨)',
        'ê²Œìž„ì˜ ë³¸ì§ˆì  ìž¬ë¯¸ ë¶€ì¡± (ê²Œìž„í”Œë ˆì´/ìŠ¤í† ë¦¬)',
        'ì½˜í…ì¸  ê³ ê°ˆ ë° ì •ì²´ (ìƒˆë¡œìš´ ì½˜í…ì¸ /ì„œë²„)',
        'ê²Œìž„ì˜ ìž¥ê¸°ì  ë¹„ì „ ë¶€ìž¬ (ì†Œí†µ/ë¯¸ëž˜)'
    ],
    'ëŒ€í‘œ í‚¤ì›Œë“œ (Topic 1)': [
        'buy, version, store, money, minutes',
        'gameplay, story, characters, hard',
        'new, enemy, server, hours',
        'community, fun, waste, longer'
    ],
    'í•µì‹¬ ë¶ˆë§Œ (Topic 2)': [
        'ê¸°ìˆ ì  ë¬¸ì œ ë° ì™„ì„±ë„',
        'ë¶ˆì¾Œí•œ ê²½í—˜ (ë‚œì´ë„/ì „íˆ¬)',
        'ë°˜ë³µ í”Œë ˆì´ì˜ í”¼ë¡œê°',
        'ì»¤ë®¤ë‹ˆí‹° ë° ìš´ì˜ ë¬¸ì œ'
    ],
    'ëŒ€í‘œ í‚¤ì›Œë“œ (Topic 2)': [
        'chinese, old, work, wrong',
        'gun, battle, hard, stop',
        'team, dragon, world, died',
        'chinese, team, mode, simulator'
    ]
}

# --- 2. ë°ì´í„°í”„ë ˆìž„ ìƒì„± ---
summary_df = pd.DataFrame(summary_data)

# --- 3. ìµœì¢… ìš”ì•½ í…Œì´ë¸” ì¶œë ¥ ---
# display() í•¨ìˆ˜ëŠ” Colabì´ë‚˜ Jupyter Notebook í™˜ê²½ì—ì„œ í‘œë¥¼ ë” ì˜ˆì˜ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.
# ì¼ë°˜ Python ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” print(summary_df)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
display(summary_df)


# =============================================================
# ðŸ“– ë°œí‘œìš© ì¶”ê°€ í•´ì„ ë° ì‹œì‚¬ì 
# =============================================================
print("\n" + "="*60)
print("ðŸ“– ë°œí‘œìš© í•´ì„ ë° ì‹œì‚¬ì ")
print("="*60)
print("1. [ì´ˆê¸° íƒìƒ‰ ê·¸ë£¹]ì€ 'êµ¬ë§¤'ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë¶ˆë§Œì´ ê°€ìž¥ ë§ŽìŠµë‹ˆë‹¤.")
print("   -> ì‹œì‚¬ì : ìŠ¤íŒ€ ìŠ¤í† ì–´ì˜ ê²Œìž„ ì„¤ëª…, ìŠ¤í¬ë¦°ìƒ·, íŠ¸ë ˆì¼ëŸ¬ê°€ ì‹¤ì œ ê²Œìž„ ê²½í—˜ê³¼ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.")
print("\n2. [ì¼ë°˜ ìœ ì € ê·¸ë£¹]ì€ 'ê²Œìž„í”Œë ˆì´', 'ìŠ¤í† ë¦¬' ë“± ê²Œìž„ì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•œ ë¹„íŒì´ ì£¼ë¥¼ ì´ë£¹ë‹ˆë‹¤.")
print("   -> ì‹œì‚¬ì : ê²Œìž„ì´ ìž¥ê¸°ì ìœ¼ë¡œ ìœ ì €ë¥¼ ë¶™ìž¡ê¸° ìœ„í•´ì„œëŠ” íƒ„íƒ„í•œ ê¸°ë³¸ê¸°ê°€ í•„ìˆ˜ì ìž…ë‹ˆë‹¤.")
print("\n3. [í—¤ë¹„ ìœ ì € ê·¸ë£¹]ì— ì™€ì„œì•¼ 'ì„œë²„(server)', 'ìƒˆë¡œìš´ ì (enemy)' ë“± ë¼ì´ë¸Œ ì„œë¹„ìŠ¤ ìš´ì˜ê³¼ ê´€ë ¨ëœ ë¶ˆë§Œì´ ë“±ìž¥í•©ë‹ˆë‹¤.")
print("   -> ì‹œì‚¬ì : ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ì™€ ê¾¸ì¤€í•œ ì½˜í…ì¸  ì—…ë°ì´íŠ¸ëŠ” ì¶©ì„±ë„ ë†’ì€ ìœ ì €ë¥¼ ìœ ì§€í•˜ëŠ” í•µì‹¬ìž…ë‹ˆë‹¤.")
print("\n4. [ì½”ì–´ ìœ ì € ê·¸ë£¹]ì€ 'ì»¤ë®¤ë‹ˆí‹°(community)', 'ìž¬ë¯¸ì˜ ì†Œë©¸(fun, waste)' ë“± ê²Œìž„ì˜ ì§€ì† ê°€ëŠ¥ì„±ê³¼ ë°©í–¥ì„±ì— ëŒ€í•´ ì´ì•¼ê¸°í•©ë‹ˆë‹¤.")
print("   -> ì‹œì‚¬ì : ì´ë“¤ì˜ ëª©ì†Œë¦¬ëŠ” ê²Œìž„ì˜ ë¯¸ëž˜ë¥¼ ê²°ì •í•˜ëŠ” ê°€ìž¥ ì¤‘ìš”í•œ ì§€í‘œì´ë©°, ê°œë°œì‚¬ëŠ” ì´ë“¤ê³¼ì˜ ì†Œí†µ ì±„ë„ì„ ë°˜ë“œì‹œ í™•ë³´í•´ì•¼ í•©ë‹ˆë‹¤.")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# =============================================================
# âœ¨ [ë¶„ì„ í†µì¼ì„± í™•ë³´] ìµœì¢… ë°ì´í„°ì…‹ ê¸°ë°˜ ê°ì„± ë¶„ì„ ë° ì‹œê°í™” âœ¨
# =============================================================

# --- 1. ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ 'df_english' ë°ì´í„°í”„ë ˆìž„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ ---
# df_english ê°€ ë©”ëª¨ë¦¬ì— ì—†ë‹¤ë©´, ì´ì „ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.

print(f"âœ… ë¶„ì„ ëŒ€ìƒ ë°ì´í„°: ì˜ì–´(ë¹„í•œêµ­ì–´) ë¦¬ë·° {len(df_english)}ê°œ")

# --- 2. VADER ê°ì„± ë¶„ì„ ìž¬ì‹¤í–‰ ---
vader_analyzer = SentimentIntensityAnalyzer()

def get_vader_score(text):
    if not isinstance(text, str): return 0.0
    return vader_analyzer.polarity_scores(text)['compound']

print("\nVADER ê°ì„± ì ìˆ˜ ê³„ì‚° ì¤‘...")
df_english['sentiment_score'] = df_english['translated_en'].progress_apply(get_vader_score)

# ê°ì„± ê·¸ë£¹ ë¶„ë¥˜ (ê¸°ì¡´ê³¼ ë™ì¼í•œ ê¸°ì¤€ ì ìš©)
def assign_sentiment_group_strict(score):
    if score >= 0.05: return 'Positive'
    elif score <= -0.3: return 'Negative'
    else: return 'Neutral'

df_english['sentiment_group'] = df_english['sentiment_score'].apply(assign_sentiment_group_strict)
print("ê°ì„± ê·¸ë£¹ ë¶„ë¥˜ ì™„ë£Œ!")
print(df_english['sentiment_group'].value_counts())


# --- 3. ê°ì„± ê·¸ë£¹ë³„ ì¶”ì²œ ë¦¬ë·° ë¹„ìœ¨ ë§‰ëŒ€ê·¸ëž˜í”„ ìž¬ì‹œê°í™” ---
sentiment_recommend_ratio = df_english.groupby('sentiment_group')['voted_up'].mean().reindex(['Positive', 'Neutral', 'Negative'])

print("\n--- [ìµœì¢… ë°ì´í„°ì…‹ ê¸°ì¤€] ê°ì„± ê·¸ë£¹ë³„ ì¶”ì²œ(voted_up=1) ë¹„ìœ¨ ---")
print(sentiment_recommend_ratio)

plt.figure(figsize=(10, 6))
sns.barplot(x=sentiment_recommend_ratio.index, y=sentiment_recommend_ratio.values, palette='viridis')
plt.title('[ìµœì¢… ë¶„ì„ ë°ì´í„° ê¸°ì¤€] ê°ì„± ê·¸ë£¹ë³„ ì¶”ì²œ ë¦¬ë·° ë¹„ìœ¨', fontsize=16, pad=20)
plt.ylabel('ì¶”ì²œ ë¹„ìœ¨ (voted_upì˜ í‰ê· )', fontsize=12)
plt.xlabel('ë¦¬ë·° í…ìŠ¤íŠ¸ì˜ ê°ì„± ê·¸ë£¹', fontsize=12)
plt.ylim(0, 1)

# ë§‰ëŒ€ ìœ„ì— ë¹„ìœ¨ í…ìŠ¤íŠ¸ í‘œì‹œ
for i, val in enumerate(sentiment_recommend_ratio):
    plt.text(i, val + 0.02, f'{val:.2%}', ha='center', fontsize=12)

plt.show()
