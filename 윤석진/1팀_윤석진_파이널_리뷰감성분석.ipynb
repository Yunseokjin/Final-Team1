{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuttCoqbAtKVkWFQ7zILEJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yunseokjin/Final-Team1/blob/main/1%ED%8C%80_%EC%9C%A4%EC%84%9D%EC%A7%84_%ED%8C%8C%EC%9D%B4%EB%84%90_%EB%A6%AC%EB%B7%B0%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad1uoPhc0ZEu"
      },
      "outputs": [],
      "source": [
        "#í•œê¸€ ê¸€ì”¨ í°íŠ¸ ì„¤ì¹˜\n",
        "%%capture\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\n",
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "\n",
        "# í‘œì—ì„œ ('-') ë§ˆì´ë„ˆìŠ¤ í‘œì‹œ\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# [Step 1] ë°ì´í„° ì¤€ë¹„: ì˜ì–´ ë¦¬ë·° í•„í„°ë§ ë° ê·¸ë£¹í•‘\n",
        "# =============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- 1-1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•„ìš”ì‹œ) ---\n",
        "!pip install -q transformers torch vaderSentiment bertopic konlpy\n",
        "\n",
        "# --- 1-2. ì›ë³¸ ë°ì´í„° ë¡œë“œ ---\n",
        "file_path = '/content/S-team_250930-final.csv'\n",
        "try:\n",
        "    df_original = pd.read_csv(file_path)\n",
        "    print(f\"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ! (Shape: {df_original.shape})\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: '{file_path}' ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "# --- 1-3. ì–¸ì–´ íƒì§€ ë° ì˜ì–´(ë¹„í•œêµ­ì–´) ë°ì´í„° í•„í„°ë§ ---\n",
        "tqdm.pandas()\n",
        "\n",
        "def detect_language(text):\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0: return 'unknown'\n",
        "    return 'korean' if re.search(\"[ã„±-íž£]\", text) else 'english'\n",
        "\n",
        "# 'translated_en' ì»¬ëŸ¼ì˜ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì–¸ì–´ íƒì§€\n",
        "df_original['language_detected'] = df_original['review'].progress_apply(detect_language)\n",
        "\n",
        "# ðŸ”¥ ë¶„ì„ ëŒ€ìƒì„ 'ì˜ë¬¸ ë¦¬ë·°'ë¡œ í•œì •\n",
        "df_english = df_original[df_original['language_detected'] != 'korean'].copy()\n",
        "print(f\"\\nâœ… ì˜ì–´(ë¹„í•œêµ­ì–´) ë¦¬ë·° í•„í„°ë§ ì™„ë£Œ! (ì´ {len(df_english)}ê°œ)\")\n",
        "\n",
        "\n",
        "# --- 1-4. ì‚¬ìš©ìž ê·¸ë£¹í•‘ ì»¬ëŸ¼ ìƒì„± ---\n",
        "# Playtime ê¸°ì¤€ ê·¸ë£¹í•‘\n",
        "df_english['playtime_hours_at_review'] = df_english['author_playtime_at_review'] / 60.0\n",
        "# 'own'ê³¼ 'trial' ê·¸ë£¹ì„ ë³‘í•©\n",
        "df_english['playtime_merged'] = df_english['user_groupby_playtime'].replace({\n",
        "    'own': 'ì´ˆê¸° íƒìƒ‰(0-10h)', 'trial': 'ì´ˆê¸° íƒìƒ‰(0-10h)',\n",
        "    'normal': 'ì¼ë°˜ ìœ ì €', 'heavy': 'í—¤ë¹„ ìœ ì €', 'core': 'ì½”ì–´ ìœ ì €'\n",
        "})\n",
        "\n",
        "print(\"\\n--- ìƒì„±ëœ í”Œë ˆì´íƒ€ìž„ ê·¸ë£¹ ë¶„í¬ ---\")\n",
        "print(df_english['playtime_merged'].value_counts())"
      ],
      "metadata": {
        "id": "2QTBJ9iH0k_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# [Step 2] ê°ì„± ë¶„ì„: VADER ëª¨ë¸ ì ìš©\n",
        "# =============================================================\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# --- 2-1. VADER ë¶„ì„ê¸° ë¡œë“œ ---\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# --- 2-2. ê°ì„± ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ ì •ì˜ ---\n",
        "def get_vader_score(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 0.0\n",
        "    # VADERëŠ” ì›ë¬¸ í…ìŠ¤íŠ¸ì— ë°”ë¡œ ì ìš©\n",
        "    return vader_analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "# --- 2-3. ê°ì„± ì ìˆ˜ ë° ê·¸ë£¹ ì»¬ëŸ¼ ìƒì„± ---\n",
        "print(\"\\nì˜ì–´ ë¦¬ë·°ì— VADER ê°ì„± ë¶„ì„ ì ìš© ì¤‘...\")\n",
        "df_english['sentiment_score'] = df_english['translated_en'].progress_apply(get_vader_score)\n",
        "\n",
        "def assign_sentiment_group_strict(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.3:  # ðŸ”¥ ë¶€ì • ê¸°ì¤€ì„ -0.05ì—ì„œ -0.3ìœ¼ë¡œ ê°•í™”\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# ê°•í™”ëœ ê¸°ì¤€ìœ¼ë¡œ ìž¬ì‹¤í–‰\n",
        "df_english['sentiment_group'] = df_english['sentiment_score'].apply(assign_sentiment_group_strict)\n",
        "\n",
        "print(\"\\nâœ… [ê°•í™”ëœ ê¸°ì¤€] ê°ì„± ë¶„ì„ ì™„ë£Œ!\")\n",
        "print(\"--- ê°ì„± ê·¸ë£¹ ë¶„í¬ ---\")\n",
        "print(df_english['sentiment_group'].value_counts())"
      ],
      "metadata": {
        "id": "zMC8tX5W0L8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# âœ¨ [ê°œì„ ëœ ì „ëžµ] Step 3: ê³ ë„í™”ëœ LDA í† í”½ ëª¨ë¸ë§ âœ¨\n",
        "# =============================================================\n",
        "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "\n",
        "# --- ì´ì „ Step 1 & 2ëŠ” ë™ì¼í•˜ê²Œ ì‹¤í–‰í•˜ì—¬ 'df_english' ë°ì´í„°í”„ë ˆìž„ì„ ì¤€ë¹„í•©ë‹ˆë‹¤ ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ¨ [ê°œì„ ëœ ë¶„ì„] ê·¸ë£¹ë³„ 'ì°¨ë³„í™”ëœ' ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ (LDA) âœ¨\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "playtime_categories = ['ì´ˆê¸° íƒìƒ‰(0-10h)', 'ì¼ë°˜ ìœ ì €', 'í—¤ë¹„ ìœ ì €', 'ì½”ì–´ ìœ ì €']\n",
        "# ê·¸ë£¹ë³„ ìµœì¢… ê²°ê³¼ë¥¼ ì €ìž¥í•  ë”•ì…”ë„ˆë¦¬\n",
        "final_results = {}\n",
        "\n",
        "# ðŸ”¥ [í•µì‹¬ ê°œì„  1] ê°•ë ¥í•œ ë¶ˆìš©ì–´ ì‚¬ì „ êµ¬ì¶•\n",
        "# ê¸°ë³¸ ì˜ì–´ ë¶ˆìš©ì–´ + ì¼ë°˜ì ì¸ ê²Œìž„ ìš©ì–´ + ì´ì „ ë¶„ì„ì—ì„œ ë°œê²¬ëœ ë¬´ì˜ë¯¸í•œ ë‹¨ì–´ ëª¨ë‘ ì¶”ê°€\n",
        "custom_stop_words = list(ENGLISH_STOP_WORDS) + [\n",
        "    'game', 'games', 'play', 'playing', 'player', 'players', 'just', 'like',\n",
        "    'really', 'don', 'doesn', 'im', 've', 'time', 'lot', 'way', 'thing',\n",
        "    'good', 'bad', 'great', 'problem', 'problems', 'issue', 'issues', 'bit', 'even',\n",
        "    'got', 'didn', 'think', 'thought', 'said', 'saw', 'know', 'want', 'make',\n",
        "    'actually', 'people', 'going', 'getting', 'little', 'much', 'review', 'steam'\n",
        "]\n",
        "# ðŸ”¥ V2: ê°ì • í‘œí˜„ ë° ë²”ìš©ì  ë¶€ì • ë‹¨ì–´ ì¶”ê°€\n",
        "custom_stop_words_v2 = custom_stop_words + [\n",
        "    'kill', 'killed', 'die', 'dead', 'death', 'hate',         # í­ë ¥/ì£½ìŒ ê´€ë ¨\n",
        "    'fuck', 'shit', 'ass', 'crap', 'motherfucking', 'bull', # ìš•ì„¤/ë¹„ì†ì–´\n",
        "    'bullying', 'bully',                                      # íŠ¹ì • ê²Œìž„ì—ì„œ ë§Žì´ ë‚˜ì˜¨ ë¶ˆìš©ì–´\n",
        "    'll', 'guy', 'guys', 'man', 'life'                         # ì¼ë°˜ì ì¸ ëª…ì‚¬/ëŒ€ëª…ì‚¬\n",
        "]\n",
        "\n",
        "for segment in playtime_categories:\n",
        "    print(f\"\\n--- ðŸ‘¥ ê·¸ë£¹: {segment} ---\")\n",
        "    segment_df = df_english[\n",
        "        (df_english['playtime_merged'] == segment) &\n",
        "        (df_english['sentiment_group'] == 'Negative') # 'Negative' ê·¸ë£¹ë§Œ ë¶„ì„\n",
        "    ]\n",
        "\n",
        "    num_docs = len(segment_df)\n",
        "    if num_docs < 50: # ë¶„ì„ì˜ ì‹ ë¢°ë„ë¥¼ ìœ„í•´ ìµœì†Œ ë¬¸ì„œ ìˆ˜ë¥¼ 50ìœ¼ë¡œ ìƒí–¥\n",
        "        print(f\"ë¶€ì • ë¦¬ë·° ìˆ˜ê°€ {num_docs}ê°œë¡œ ë¶„ì„í•˜ê¸°ì— ë¶€ì¡±í•©ë‹ˆë‹¤.\")\n",
        "        continue\n",
        "\n",
        "    docs = segment_df['translated_en'].dropna().astype(str).tolist()\n",
        "    print(f\"ì´ {len(docs)}ê°œì˜ ì˜ì–´ ë¶€ì • ë¦¬ë·°ë¡œ LDA í† í”½ ëª¨ë¸ë§ ì‹œìž‘...\")\n",
        "\n",
        "    try:\n",
        "        # ðŸ”¥ [í•µì‹¬ ê°œì„  2] Vectorizer íŒŒë¼ë¯¸í„° ìµœì í™”\n",
        "        # min_df: ë„ˆë¬´ ë“œë¬¼ê²Œ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ ì œê±° (ìµœì†Œ 10ê°œ ë¬¸ì„œ ì´ìƒ ë“±ìž¥)\n",
        "        # max_df: ë„ˆë¬´ ìžì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ ì œê±° (ìƒìœ„ 85% ì´ìƒ ë“±ìž¥ ë‹¨ì–´ ì œì™¸)\n",
        "        # ngram_range: ë‹¨ì–´ ë¬¶ìŒ(ì˜ˆ: 'server connection')ì„ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ ì •í™•ë„ í–¥ìƒ\n",
        "        vectorizer = CountVectorizer(\n",
        "            stop_words=custom_stop_words_v2,\n",
        "            min_df=10,\n",
        "            max_df=0.85,\n",
        "            ngram_range=(1, 2) # 1ê°œ ë‹¨ì–´ ë° 2ê°œ ì—°ì† ë‹¨ì–´ ëª¨ë‘ ê³ ë ¤\n",
        "        )\n",
        "        X = vectorizer.fit_transform(docs)\n",
        "\n",
        "        # ðŸ”¥ [í•µì‹¬ ê°œì„  3] ì°¾ê³ ìž í•˜ëŠ” í† í”½ì˜ ìˆ˜ë¥¼ 3~5ê°œë¡œ ì¤„ì—¬ ëª…í™•ì„± í™•ë³´\n",
        "        lda_model = LatentDirichletAllocation(\n",
        "            n_components=4, # í•µì‹¬ ë¶ˆë§Œ 4ê°œë¥¼ ì°¾ëŠ”ë‹¤ê³  ê°€ì •\n",
        "            random_state=42,\n",
        "            learning_method='online', # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ë” ë¹ ë¦„\n",
        "            n_jobs=-1 # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©\n",
        "        )\n",
        "        lda_model.fit(X)\n",
        "\n",
        "        # 3. ê²°ê³¼ í•´ì„ ë° ì €ìž¥\n",
        "        topics = []\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        for topic_idx, topic in enumerate(lda_model.components_):\n",
        "            top_keywords = [feature_names[i] for i in topic.argsort()[:-8:-1]] # ìƒìœ„ 7ê°œ í‚¤ì›Œë“œ\n",
        "            topics.append({\n",
        "                \"Topic #\": topic_idx,\n",
        "                \"Keywords\": \", \".join(top_keywords)\n",
        "            })\n",
        "\n",
        "        results_df = pd.DataFrame(topics)\n",
        "        final_results[segment] = results_df\n",
        "        print(\"\\nðŸ”¥ ë°œê²¬ëœ ì£¼ìš” ë¶ˆë§Œ í† í”½:\")\n",
        "        display(results_df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ LDA ëª¨ë¸ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
      ],
      "metadata": {
        "id": "_Inoz7URzkVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# âœ¨ [ìµœì¢… ë‹¨ê³„] ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë° í•´ì„ âœ¨\n",
        "# =============================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"âœ¨ ì „ì²´ ê·¸ë£¹ë³„ ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ ìš”ì•½ âœ¨\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ì´ì „ LDA ë¶„ì„ì—ì„œ ë„ì¶œëœ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° í† í”½ì— ì˜ë¯¸(ë ˆì´ë¸”)ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n",
        "# ì´ í•´ì„ì€ ë¶„ì„ê°€ì˜ ì£¼ê´€ì´ ë“¤ì–´ê°€ëŠ” ë¶€ë¶„ì´ë©°, ë°œí‘œì˜ í•µì‹¬ì ì¸ ì£¼ìž¥ìž…ë‹ˆë‹¤.\n",
        "\n",
        "# --- 1. ê° ê·¸ë£¹ë³„ í‚¤ì›Œë“œ í•´ì„ ë° ë ˆì´ë¸”ë§ ---\n",
        "summary_data = {\n",
        "    'í”Œë ˆì´íƒ€ìž„ ê·¸ë£¹': [\n",
        "        'ì´ˆê¸° íƒìƒ‰ (0-10h)',\n",
        "        'ì¼ë°˜ ìœ ì € (10-100h)',\n",
        "        'í—¤ë¹„ ìœ ì € (100h+)',\n",
        "        'ì½”ì–´ ìœ ì € (ìˆ˜ë°± ì‹œê°„+)'\n",
        "    ],\n",
        "    'í•µì‹¬ ë¶ˆë§Œ (Topic 1)': [\n",
        "        'ê¸°ëŒ€ì™€ ë‹¤ë¥¸ ì²«ì¸ìƒ (êµ¬ë§¤/ê°€ì¹˜ íŒë‹¨)',\n",
        "        'ê²Œìž„ì˜ ë³¸ì§ˆì  ìž¬ë¯¸ ë¶€ì¡± (ê²Œìž„í”Œë ˆì´/ìŠ¤í† ë¦¬)',\n",
        "        'ì½˜í…ì¸  ê³ ê°ˆ ë° ì •ì²´ (ìƒˆë¡œìš´ ì½˜í…ì¸ /ì„œë²„)',\n",
        "        'ê²Œìž„ì˜ ìž¥ê¸°ì  ë¹„ì „ ë¶€ìž¬ (ì†Œí†µ/ë¯¸ëž˜)'\n",
        "    ],\n",
        "    'ëŒ€í‘œ í‚¤ì›Œë“œ (Topic 1)': [\n",
        "        'buy, version, store, money, minutes',\n",
        "        'gameplay, story, characters, hard',\n",
        "        'new, enemy, server, hours',\n",
        "        'community, fun, waste, longer'\n",
        "    ],\n",
        "    'í•µì‹¬ ë¶ˆë§Œ (Topic 2)': [\n",
        "        'ê¸°ìˆ ì  ë¬¸ì œ ë° ì™„ì„±ë„',\n",
        "        'ë¶ˆì¾Œí•œ ê²½í—˜ (ë‚œì´ë„/ì „íˆ¬)',\n",
        "        'ë°˜ë³µ í”Œë ˆì´ì˜ í”¼ë¡œê°',\n",
        "        'ì»¤ë®¤ë‹ˆí‹° ë° ìš´ì˜ ë¬¸ì œ'\n",
        "    ],\n",
        "    'ëŒ€í‘œ í‚¤ì›Œë“œ (Topic 2)': [\n",
        "        'chinese, old, work, wrong',\n",
        "        'gun, battle, hard, stop',\n",
        "        'team, dragon, world, died',\n",
        "        'chinese, team, mode, simulator'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# --- 2. ë°ì´í„°í”„ë ˆìž„ ìƒì„± ---\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# --- 3. ìµœì¢… ìš”ì•½ í…Œì´ë¸” ì¶œë ¥ ---\n",
        "# display() í•¨ìˆ˜ëŠ” Colabì´ë‚˜ Jupyter Notebook í™˜ê²½ì—ì„œ í‘œë¥¼ ë” ì˜ˆì˜ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
        "# ì¼ë°˜ Python ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” print(summary_df)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n",
        "display(summary_df)\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# ðŸ“– ë°œí‘œìš© ì¶”ê°€ í•´ì„ ë° ì‹œì‚¬ì \n",
        "# =============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“– ë°œí‘œìš© í•´ì„ ë° ì‹œì‚¬ì \")\n",
        "print(\"=\"*60)\n",
        "print(\"1. [ì´ˆê¸° íƒìƒ‰ ê·¸ë£¹]ì€ 'êµ¬ë§¤'ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë¶ˆë§Œì´ ê°€ìž¥ ë§ŽìŠµë‹ˆë‹¤.\")\n",
        "print(\"   -> ì‹œì‚¬ì : ìŠ¤íŒ€ ìŠ¤í† ì–´ì˜ ê²Œìž„ ì„¤ëª…, ìŠ¤í¬ë¦°ìƒ·, íŠ¸ë ˆì¼ëŸ¬ê°€ ì‹¤ì œ ê²Œìž„ ê²½í—˜ê³¼ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.\")\n",
        "print(\"\\n2. [ì¼ë°˜ ìœ ì € ê·¸ë£¹]ì€ 'ê²Œìž„í”Œë ˆì´', 'ìŠ¤í† ë¦¬' ë“± ê²Œìž„ì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•œ ë¹„íŒì´ ì£¼ë¥¼ ì´ë£¹ë‹ˆë‹¤.\")\n",
        "print(\"   -> ì‹œì‚¬ì : ê²Œìž„ì´ ìž¥ê¸°ì ìœ¼ë¡œ ìœ ì €ë¥¼ ë¶™ìž¡ê¸° ìœ„í•´ì„œëŠ” íƒ„íƒ„í•œ ê¸°ë³¸ê¸°ê°€ í•„ìˆ˜ì ìž…ë‹ˆë‹¤.\")\n",
        "print(\"\\n3. [í—¤ë¹„ ìœ ì € ê·¸ë£¹]ì— ì™€ì„œì•¼ 'ì„œë²„(server)', 'ìƒˆë¡œìš´ ì (enemy)' ë“± ë¼ì´ë¸Œ ì„œë¹„ìŠ¤ ìš´ì˜ê³¼ ê´€ë ¨ëœ ë¶ˆë§Œì´ ë“±ìž¥í•©ë‹ˆë‹¤.\")\n",
        "print(\"   -> ì‹œì‚¬ì : ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ì™€ ê¾¸ì¤€í•œ ì½˜í…ì¸  ì—…ë°ì´íŠ¸ëŠ” ì¶©ì„±ë„ ë†’ì€ ìœ ì €ë¥¼ ìœ ì§€í•˜ëŠ” í•µì‹¬ìž…ë‹ˆë‹¤.\")\n",
        "print(\"\\n4. [ì½”ì–´ ìœ ì € ê·¸ë£¹]ì€ 'ì»¤ë®¤ë‹ˆí‹°(community)', 'ìž¬ë¯¸ì˜ ì†Œë©¸(fun, waste)' ë“± ê²Œìž„ì˜ ì§€ì† ê°€ëŠ¥ì„±ê³¼ ë°©í–¥ì„±ì— ëŒ€í•´ ì´ì•¼ê¸°í•©ë‹ˆë‹¤.\")\n",
        "print(\"   -> ì‹œì‚¬ì : ì´ë“¤ì˜ ëª©ì†Œë¦¬ëŠ” ê²Œìž„ì˜ ë¯¸ëž˜ë¥¼ ê²°ì •í•˜ëŠ” ê°€ìž¥ ì¤‘ìš”í•œ ì§€í‘œì´ë©°, ê°œë°œì‚¬ëŠ” ì´ë“¤ê³¼ì˜ ì†Œí†µ ì±„ë„ì„ ë°˜ë“œì‹œ í™•ë³´í•´ì•¼ í•©ë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "FtugtPGx0DJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# =============================================================\n",
        "# âœ¨ [ë¶„ì„ í†µì¼ì„± í™•ë³´] ìµœì¢… ë°ì´í„°ì…‹ ê¸°ë°˜ ê°ì„± ë¶„ì„ ë° ì‹œê°í™” âœ¨\n",
        "# =============================================================\n",
        "\n",
        "# --- 1. ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ 'df_english' ë°ì´í„°í”„ë ˆìž„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ ---\n",
        "# df_english ê°€ ë©”ëª¨ë¦¬ì— ì—†ë‹¤ë©´, ì´ì „ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.\n",
        "\n",
        "print(f\"âœ… ë¶„ì„ ëŒ€ìƒ ë°ì´í„°: ì˜ì–´(ë¹„í•œêµ­ì–´) ë¦¬ë·° {len(df_english)}ê°œ\")\n",
        "\n",
        "# --- 2. VADER ê°ì„± ë¶„ì„ ìž¬ì‹¤í–‰ ---\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_vader_score(text):\n",
        "    if not isinstance(text, str): return 0.0\n",
        "    return vader_analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "print(\"\\nVADER ê°ì„± ì ìˆ˜ ê³„ì‚° ì¤‘...\")\n",
        "df_english['sentiment_score'] = df_english['translated_en'].progress_apply(get_vader_score)\n",
        "\n",
        "# ê°ì„± ê·¸ë£¹ ë¶„ë¥˜ (ê¸°ì¡´ê³¼ ë™ì¼í•œ ê¸°ì¤€ ì ìš©)\n",
        "def assign_sentiment_group_strict(score):\n",
        "    if score >= 0.05: return 'Positive'\n",
        "    elif score <= -0.3: return 'Negative'\n",
        "    else: return 'Neutral'\n",
        "\n",
        "df_english['sentiment_group'] = df_english['sentiment_score'].apply(assign_sentiment_group_strict)\n",
        "print(\"ê°ì„± ê·¸ë£¹ ë¶„ë¥˜ ì™„ë£Œ!\")\n",
        "print(df_english['sentiment_group'].value_counts())\n",
        "\n",
        "\n",
        "# --- 3. ê°ì„± ê·¸ë£¹ë³„ ì¶”ì²œ ë¦¬ë·° ë¹„ìœ¨ ë§‰ëŒ€ê·¸ëž˜í”„ ìž¬ì‹œê°í™” ---\n",
        "sentiment_recommend_ratio = df_english.groupby('sentiment_group')['voted_up'].mean().reindex(['Positive', 'Neutral', 'Negative'])\n",
        "\n",
        "print(\"\\n--- [ìµœì¢… ë°ì´í„°ì…‹ ê¸°ì¤€] ê°ì„± ê·¸ë£¹ë³„ ì¶”ì²œ(voted_up=1) ë¹„ìœ¨ ---\")\n",
        "print(sentiment_recommend_ratio)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=sentiment_recommend_ratio.index, y=sentiment_recommend_ratio.values, palette='viridis')\n",
        "plt.title('[ìµœì¢… ë¶„ì„ ë°ì´í„° ê¸°ì¤€] ê°ì„± ê·¸ë£¹ë³„ ì¶”ì²œ ë¦¬ë·° ë¹„ìœ¨', fontsize=16, pad=20)\n",
        "plt.ylabel('ì¶”ì²œ ë¹„ìœ¨ (voted_upì˜ í‰ê· )', fontsize=12)\n",
        "plt.xlabel('ë¦¬ë·° í…ìŠ¤íŠ¸ì˜ ê°ì„± ê·¸ë£¹', fontsize=12)\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# ë§‰ëŒ€ ìœ„ì— ë¹„ìœ¨ í…ìŠ¤íŠ¸ í‘œì‹œ\n",
        "for i, val in enumerate(sentiment_recommend_ratio):\n",
        "    plt.text(i, val + 0.02, f'{val:.2%}', ha='center', fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZZ3k8LZ_0XMS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
